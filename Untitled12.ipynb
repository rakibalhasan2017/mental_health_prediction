{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-p5bDzL6Tp3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, logging as hf_logging\n",
        "\n",
        "hf_logging.set_verbosity_error()  # silence some HF logs\n",
        "\n",
        "# ------------------- Reproducibility -------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ------------------- 1. Load & Clean Dataset -------------------\n",
        "df = pd.read_csv(\"/content/Mental Health Disorder Detection Dataset.csv\")\n",
        "df.dropna(subset=['body', 'category'], inplace=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['body'] = df['body'].apply(clean_text)\n",
        "\n",
        "# Encode labels\n",
        "label_enc = LabelEncoder()\n",
        "df['category'] = label_enc.fit_transform(df['category'])\n",
        "num_classes = len(label_enc.classes_)\n",
        "\n",
        "# initial train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['body'], df['category'], test_size=0.2, stratify=df['category'], random_state=SEED\n",
        ")\n",
        "\n",
        "# create validation set from train\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, stratify=train_labels, random_state=SEED\n",
        ")\n",
        "\n",
        "# ------------------- 2. LSTM (tokenizer + sequences) -------------------\n",
        "MAX_WORDS = 20000\n",
        "MAX_LEN_LSTM = 100\n",
        "\n",
        "tokenizer_lstm = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer_lstm.fit_on_texts(train_texts)\n",
        "\n",
        "def texts_to_padded_tensor(texts):\n",
        "    seqs = tokenizer_lstm.texts_to_sequences(texts)\n",
        "    padded = pad_sequences(seqs, maxlen=MAX_LEN_LSTM, padding='post')\n",
        "    return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "X_train_lstm = texts_to_padded_tensor(list(train_texts))\n",
        "X_val_lstm = texts_to_padded_tensor(list(val_texts))\n",
        "X_test_lstm = texts_to_padded_tensor(list(test_texts))\n",
        "\n",
        "y_train_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(val_labels.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "# ------------------- 3. GloVe embeddings -------------------\n",
        "# Download \"glove.6B.100d.txt\" and provide path\n",
        "GLOVE_PATH = \"glove.6B.100d.txt\"\n",
        "EMBED_DIM = 100\n",
        "\n",
        "embedding_index = {}\n",
        "with open(GLOVE_PATH, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "word_index = tokenizer_lstm.word_index\n",
        "vocab_size = min(MAX_WORDS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((vocab_size, EMBED_DIM), dtype=np.float32)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        vec = embedding_index.get(word)\n",
        "        if vec is not None:\n",
        "            embedding_matrix[i] = vec\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix)  # float32\n",
        "\n",
        "# ------------------- 4. DistilBERT preprocessing -------------------\n",
        "MAX_LEN_BERT = 128\n",
        "tokenizer_bert = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def encode_texts(texts):\n",
        "    # return PyTorch tensors\n",
        "    return tokenizer_bert(list(texts), truncation=True, padding='max_length',\n",
        "                          max_length=MAX_LEN_BERT, return_tensors=\"pt\")\n",
        "\n",
        "train_encodings = encode_texts(train_texts)\n",
        "val_encodings = encode_texts(val_texts)\n",
        "test_encodings = encode_texts(test_texts)\n",
        "\n",
        "# ------------------- 5. Dataset classes -------------------\n",
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
        "        return item, self.labels[idx]\n",
        "\n",
        "BATCH_LSTM = 32\n",
        "BATCH_BERT = 16\n",
        "\n",
        "train_loader_lstm = DataLoader(LSTMDataset(X_train_lstm, y_train_tensor), batch_size=BATCH_LSTM, shuffle=True)\n",
        "val_loader_lstm = DataLoader(LSTMDataset(X_val_lstm, y_val_tensor), batch_size=BATCH_LSTM)\n",
        "test_loader_lstm = DataLoader(LSTMDataset(X_test_lstm, y_test_tensor), batch_size=BATCH_LSTM)\n",
        "\n",
        "train_loader_bert = DataLoader(BERTDataset(train_encodings, y_train_tensor), batch_size=BATCH_BERT, shuffle=True)\n",
        "val_loader_bert = DataLoader(BERTDataset(val_encodings, y_val_tensor), batch_size=BATCH_BERT)\n",
        "test_loader_bert = DataLoader(BERTDataset(test_encodings, y_test_tensor), batch_size=BATCH_BERT)\n",
        "\n",
        "# ------------------- 6. Models -------------------\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, num_classes, num_layers=2, bidirectional=True, freeze_embeddings=False):\n",
        "        super().__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "        # use Embedding.from_pretrained so we can choose freeze\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings, padding_idx=0)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        _, (h_n, _) = self.lstm(emb)  # h_n: (num_layers * num_directions, batch, hidden_dim)\n",
        "        # reshape to (num_layers, num_directions, batch, hidden_dim)\n",
        "        h_n = h_n.view(self.num_layers, self.num_directions, h_n.size(1), self.hidden_dim)\n",
        "        last_layer = h_n[-1]  # (num_directions, batch, hidden_dim)\n",
        "        if self.bidirectional:\n",
        "            # concat forward and backward\n",
        "            last = torch.cat([last_layer[0], last_layer[1]], dim=1)  # (batch, hidden_dim*2)\n",
        "        else:\n",
        "            last = last_layer[0]  # (batch, hidden_dim)\n",
        "        return self.fc(last)  # (batch, num_classes)\n",
        "\n",
        "class DistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DistilBERT does not have pooled_output; we can use first token's hidden state (CLS-like)\n",
        "        hidden_state = outputs.last_hidden_state[:, 0]  # (batch, hidden_size)\n",
        "        return self.fc(hidden_state)\n",
        "\n",
        "# ------------------- 7. Initialize models, loss, optimizers, schedulers -------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# LSTM: allow embeddings to be fine-tuned (freeze_embeddings=False)\n",
        "model_lstm = LSTMClassifier(embedding_matrix, hidden_dim=128, num_classes=num_classes,\n",
        "                            num_layers=2, bidirectional=True, freeze_embeddings=False).to(device)\n",
        "model_bert = DistilBERTClassifier(num_classes=num_classes, freeze_bert=False).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=1e-3)\n",
        "optimizer_bert = optim.Adam(model_bert.parameters(), lr=2e-5)\n",
        "\n",
        "# LR schedulers (ReduceLROnPlateau based on validation loss)\n",
        "scheduler_lstm = optim.lr_scheduler.ReduceLROnPlateau(optimizer_lstm, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "scheduler_bert = optim.lr_scheduler.ReduceLROnPlateau(optimizer_bert, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "\n",
        "# ------------------- 8. Training / evaluation helpers -------------------\n",
        "def evaluate_lstm(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            total_loss += loss.item() * X_batch.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += X_batch.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate_bert(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "# Utility to get probabilities for ensemble (softmax)\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def predict_proba_lstm(model, data_loader, device):\n",
        "    model.eval()\n",
        "    probs_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            probs = softmax(outputs).cpu().numpy()\n",
        "            probs_list.append(probs)\n",
        "            labels_list.append(y_batch.numpy())\n",
        "    return np.vstack(probs_list), np.concatenate(labels_list)\n",
        "\n",
        "def predict_proba_bert(model, data_loader, device):\n",
        "    model.eval()\n",
        "    probs_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "            probs = softmax(outputs).cpu().numpy()\n",
        "            probs_list.append(probs)\n",
        "            labels_list.append(labels.numpy())\n",
        "    return np.vstack(probs_list), np.concatenate(labels_list)\n",
        "\n",
        "# ------------------- 9. Train loops with early stopping -------------------\n",
        "EPOCHS_LSTM = 10\n",
        "EPOCHS_BERT = 4\n",
        "PATIENCE = 3  # early stopping patience based on val loss\n",
        "CLIP_NORM = 1.0\n",
        "\n",
        "best_val_loss_lstm = float('inf')\n",
        "best_val_loss_bert = float('inf')\n",
        "patience_counter_lstm = 0\n",
        "patience_counter_bert = 0\n",
        "\n",
        "# Checkpoint paths\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "path_lstm = \"checkpoints/best_lstm.pt\"\n",
        "path_bert = \"checkpoints/best_bert.pt\"\n",
        "\n",
        "# Train LSTM\n",
        "for epoch in range(EPOCHS_LSTM):\n",
        "    model_lstm.train()\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader_lstm:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        optimizer_lstm.zero_grad()\n",
        "        outputs = model_lstm(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_lstm.parameters(), CLIP_NORM)\n",
        "        optimizer_lstm.step()\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "    train_loss = total_loss / len(train_loader_lstm.dataset)\n",
        "    val_loss, val_acc = evaluate_lstm(model_lstm, val_loader_lstm, device)\n",
        "    print(f\"[LSTM] Epoch {epoch+1} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    scheduler_lstm.step(val_loss)\n",
        "    # early stopping & checkpoint\n",
        "    if val_loss < best_val_loss_lstm - 1e-6:\n",
        "        best_val_loss_lstm = val_loss\n",
        "        torch.save(model_lstm.state_dict(), path_lstm)\n",
        "        patience_counter_lstm = 0\n",
        "        print(\"  -> Saved best LSTM model\")\n",
        "    else:\n",
        "        patience_counter_lstm += 1\n",
        "        if patience_counter_lstm >= PATIENCE:\n",
        "            print(\"  -> Early stopping LSTM\")\n",
        "            break\n",
        "\n",
        "# load best LSTM\n",
        "model_lstm.load_state_dict(torch.load(path_lstm))\n",
        "\n",
        "# Train BERT\n",
        "for epoch in range(EPOCHS_BERT):\n",
        "    model_bert.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader_bert:\n",
        "        inputs, labels = batch\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "        optimizer_bert.zero_grad()\n",
        "        outputs = model_bert(inputs['input_ids'], inputs['attention_mask'])\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), CLIP_NORM)\n",
        "        optimizer_bert.step()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "    train_loss = total_loss / len(train_loader_bert.dataset)\n",
        "    val_loss, val_acc = evaluate_bert(model_bert, val_loader_bert, device)\n",
        "    print(f\"[BERT] Epoch {epoch+1} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    scheduler_bert.step(val_loss)\n",
        "    # early stopping & checkpoint\n",
        "    if val_loss < best_val_loss_bert - 1e-6:\n",
        "        best_val_loss_bert = val_loss\n",
        "        torch.save(model_bert.state_dict(), path_bert)\n",
        "        patience_counter_bert = 0\n",
        "        print(\"  -> Saved best BERT model\")\n",
        "    else:\n",
        "        patience_counter_bert += 1\n",
        "        if patience_counter_bert >= PATIENCE:\n",
        "            print(\"  -> Early stopping BERT\")\n",
        "            break\n",
        "\n",
        "# load best BERT\n",
        "model_bert.load_state_dict(torch.load(path_bert))\n",
        "\n",
        "# ------------------- 10. Find best ensemble weight on validation set -------------------\n",
        "# get probabilities for val set\n",
        "lstm_val_probs, val_labels_arr = predict_proba_lstm(model_lstm, val_loader_lstm, device)\n",
        "bert_val_probs, _ = predict_proba_bert(model_bert, val_loader_bert, device)\n",
        "\n",
        "# grid search for best weight w in [0,1] -> final = w*lstm + (1-w)*bert\n",
        "best_w = None\n",
        "best_acc = -1.0\n",
        "for w in np.linspace(0.0, 1.0, 21):\n",
        "    final_probs = w * lstm_val_probs + (1.0 - w) * bert_val_probs\n",
        "    preds = final_probs.argmax(axis=1)\n",
        "    acc = (preds == val_labels_arr).mean()\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_w = w\n",
        "print(f\"Best ensemble weight on validation: w_lstm = {best_w:.2f}, val_acc = {best_acc:.4f}\")\n",
        "\n",
        "# ------------------- 11. Evaluate ensemble on test set -------------------\n",
        "lstm_test_probs, test_labels_arr = predict_proba_lstm(model_lstm, test_loader_lstm, device)\n",
        "bert_test_probs, _ = predict_proba_bert(model_bert, test_loader_bert, device)\n",
        "\n",
        "final_test_probs = best_w * lstm_test_probs + (1.0 - best_w) * bert_test_probs\n",
        "test_preds = final_test_probs.argmax(axis=1)\n",
        "test_acc = (test_preds == test_labels_arr).mean()\n",
        "print(f\"Ensemble Test Accuracy (w_lstm={best_w:.2f}): {100.0 * test_acc:.2f}%\")\n",
        "\n",
        "# Optional: show per-model test accuracies\n",
        "_, lstm_test_acc = evaluate_lstm(model_lstm, test_loader_lstm, device)\n",
        "_, bert_test_acc = evaluate_bert(model_bert, test_loader_bert, device)\n",
        "print(f\"LSTM Test Acc: {100.0 * lstm_test_acc:.2f}%\")\n",
        "print(f\"BERT Test Acc: {100.0 * bert_test_acc:.2f}%\")\n"
      ]
    }
  ]
}