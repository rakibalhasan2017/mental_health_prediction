{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakibalhasan2017/mental_health_prediction/blob/shahriar/shahriar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-p5bDzL6Tp3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, logging as hf_logging\n",
        "\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "# ------------------- Enhanced Reproducibility -------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ------------------- 1. Enhanced Data Loading & Preprocessing -------------------\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Enhanced data loading with better preprocessing\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # More thorough data cleaning\n",
        "    df = df.dropna(subset=['body', 'category'])\n",
        "    df = df[df['body'].str.len() > 10]  # Remove very short texts\n",
        "    df = df[df['body'].str.len() < 2000]  # Remove extremely long texts\n",
        "\n",
        "    print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "    print(f\"Class distribution:\\n{df['category'].value_counts()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def enhanced_clean_text(text):\n",
        "    \"\"\"Enhanced text cleaning\"\"\"\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Replace common contractions\n",
        "    contractions = {\n",
        "        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "        \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
        "        \"'m\": \" am\", \"i'm\": \"i am\", \"you're\": \"you are\"\n",
        "    }\n",
        "    for contraction, expansion in contractions.items():\n",
        "        text = text.replace(contraction, expansion)\n",
        "\n",
        "    # Keep important punctuation that might indicate mental state\n",
        "    text = re.sub(r'[^\\w\\s\\.\\!\\?]', ' ', text)\n",
        "    text = re.sub(r'\\.+', '.', text)  # Multiple periods to single\n",
        "    text = re.sub(r'\\!+', '!', text)  # Multiple exclamations to single\n",
        "    text = re.sub(r'\\?+', '?', text)  # Multiple questions to single\n",
        "\n",
        "    # Clean up whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load dataset\n",
        "df = load_and_preprocess_data(\"/content/Mental Health Disorder Detection Dataset.csv\")\n",
        "df['body'] = df['body'].apply(enhanced_clean_text)\n",
        "\n",
        "# Enhanced label encoding with class weights\n",
        "label_enc = LabelEncoder()\n",
        "df['category'] = label_enc.fit_transform(df['category'])\n",
        "num_classes = len(label_enc.classes_)\n",
        "\n",
        "# Calculate class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df['category']), y=df['category'])\n",
        "class_weight_dict = dict(zip(np.unique(df['category']), class_weights))\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Stratified split with larger validation set\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['body'], df['category'], test_size=0.3, stratify=df['category'], random_state=SEED\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=SEED\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
        "\n",
        "# ------------------- 2. Enhanced LSTM Preprocessing -------------------\n",
        "MAX_WORDS = 30000  # Increased vocabulary\n",
        "MAX_LEN_LSTM = 150  # Increased sequence length\n",
        "\n",
        "# Enhanced tokenizer with better parameters\n",
        "tokenizer_lstm = Tokenizer(\n",
        "    num_words=MAX_WORDS,\n",
        "    oov_token=\"<OOV>\",\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',  # Keep some punctuation\n",
        "    lower=True,\n",
        "    split=' '\n",
        ")\n",
        "tokenizer_lstm.fit_on_texts(train_texts)\n",
        "\n",
        "def texts_to_padded_tensor(texts):\n",
        "    seqs = tokenizer_lstm.texts_to_sequences(texts)\n",
        "    padded = pad_sequences(seqs, maxlen=MAX_LEN_LSTM, padding='post', truncating='post')\n",
        "    return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "X_train_lstm = texts_to_padded_tensor(list(train_texts))\n",
        "X_val_lstm = texts_to_padded_tensor(list(val_texts))\n",
        "X_test_lstm = texts_to_padded_tensor(list(test_texts))\n",
        "\n",
        "y_train_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(val_labels.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "# ------------------- 3. Enhanced GloVe Loading -------------------\n",
        "def load_glove_embeddings(glove_path, embed_dim=100):\n",
        "    \"\"\"Load GloVe embeddings with error handling\"\"\"\n",
        "    embedding_index = {}\n",
        "    try:\n",
        "        with open(glove_path, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "                embedding_index[word] = coefs\n",
        "        print(f\"Loaded {len(embedding_index)} word vectors.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"GloVe file not found. Using random embeddings.\")\n",
        "        return None\n",
        "    return embedding_index\n",
        "\n",
        "GLOVE_PATH = \"glove.6B.100d.txt\"\n",
        "EMBED_DIM = 100\n",
        "\n",
        "embedding_index = load_glove_embeddings(GLOVE_PATH, EMBED_DIM)\n",
        "\n",
        "# Create embedding matrix\n",
        "word_index = tokenizer_lstm.word_index\n",
        "vocab_size = min(MAX_WORDS, len(word_index) + 1)\n",
        "embedding_matrix = np.random.normal(0, 0.1, (vocab_size, EMBED_DIM)).astype(np.float32)\n",
        "\n",
        "if embedding_index:\n",
        "    found_words = 0\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:\n",
        "            vec = embedding_index.get(word)\n",
        "            if vec is not None:\n",
        "                embedding_matrix[i] = vec\n",
        "                found_words += 1\n",
        "    print(f\"Found embeddings for {found_words}/{min(len(word_index), vocab_size)} words\")\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix)\n",
        "\n",
        "# ------------------- 4. Enhanced DistilBERT Preprocessing -------------------\n",
        "MAX_LEN_BERT = 256  # Increased for better context\n",
        "tokenizer_bert = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def encode_texts(texts, max_length=MAX_LEN_BERT):\n",
        "    return tokenizer_bert(\n",
        "        list(texts),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_encodings = encode_texts(train_texts)\n",
        "val_encodings = encode_texts(val_texts)\n",
        "test_encodings = encode_texts(test_texts)\n",
        "\n",
        "# ------------------- 5. Enhanced Dataset Classes -------------------\n",
        "class EnhancedLSTMDataset(Dataset):\n",
        "    def __init__(self, X, y, weights=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.weights = weights\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.weights is not None:\n",
        "            return self.X[idx], self.y[idx], self.weights[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class EnhancedBERTDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
        "        return item, self.labels[idx]\n",
        "\n",
        "# Create weighted sampler for imbalanced data\n",
        "def create_weighted_sampler(labels):\n",
        "    class_counts = Counter(labels.numpy())\n",
        "    weights = [1.0/class_counts[label] for label in labels.numpy()]\n",
        "    return WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "# Enhanced data loaders\n",
        "BATCH_LSTM = 64  # Increased batch size\n",
        "BATCH_BERT = 32  # Increased batch size\n",
        "\n",
        "train_sampler = create_weighted_sampler(y_train_tensor)\n",
        "\n",
        "train_loader_lstm = DataLoader(\n",
        "    EnhancedLSTMDataset(X_train_lstm, y_train_tensor),\n",
        "    batch_size=BATCH_LSTM,\n",
        "    sampler=train_sampler\n",
        ")\n",
        "val_loader_lstm = DataLoader(EnhancedLSTMDataset(X_val_lstm, y_val_tensor), batch_size=BATCH_LSTM)\n",
        "test_loader_lstm = DataLoader(EnhancedLSTMDataset(X_test_lstm, y_test_tensor), batch_size=BATCH_LSTM)\n",
        "\n",
        "train_loader_bert = DataLoader(\n",
        "    EnhancedBERTDataset(train_encodings, y_train_tensor),\n",
        "    batch_size=BATCH_BERT,\n",
        "    sampler=train_sampler\n",
        ")\n",
        "val_loader_bert = DataLoader(EnhancedBERTDataset(val_encodings, y_val_tensor), batch_size=BATCH_BERT)\n",
        "test_loader_bert = DataLoader(EnhancedBERTDataset(test_encodings, y_test_tensor), batch_size=BATCH_BERT)\n",
        "\n",
        "# ------------------- 6. Enhanced Models -------------------\n",
        "class EnhancedLSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, num_classes, num_layers=2,\n",
        "                 bidirectional=True, dropout=0.3, freeze_embeddings=False):\n",
        "        super().__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings, padding_idx=0)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Enhanced LSTM with dropout\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim, hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Multi-layer classifier with dropout\n",
        "        lstm_output_dim = hidden_dim * self.num_directions\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(lstm_output_dim, lstm_output_dim // 2)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(lstm_output_dim // 2, num_classes)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm1d(lstm_output_dim // 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, (h_n, _) = self.lstm(emb)\n",
        "\n",
        "        # Use both final hidden state and attention-weighted average\n",
        "        # Final hidden state\n",
        "        h_n = h_n.view(self.num_layers, self.num_directions, h_n.size(1), self.hidden_dim)\n",
        "        last_layer = h_n[-1]  # (num_directions, batch, hidden_dim)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            last = torch.cat([last_layer[0], last_layer[1]], dim=1)  # (batch, hidden_dim*2)\n",
        "        else:\n",
        "            last = last_layer[0]  # (batch, hidden_dim)\n",
        "\n",
        "        # Multi-layer classifier\n",
        "        x = self.dropout1(last)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class EnhancedDistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout=0.3, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Multi-layer classifier\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, num_classes)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size // 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Use CLS token representation\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]  # (batch, hidden_size)\n",
        "\n",
        "        # Multi-layer classifier\n",
        "        x = self.dropout1(pooled_output)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ------------------- 7. Enhanced Training Setup -------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize enhanced models\n",
        "model_lstm = EnhancedLSTMClassifier(\n",
        "    embedding_matrix,\n",
        "    hidden_dim=256,  # Increased hidden dimension\n",
        "    num_classes=num_classes,\n",
        "    num_layers=3,  # Increased layers\n",
        "    bidirectional=True,\n",
        "    dropout=0.4,\n",
        "    freeze_embeddings=False\n",
        ").to(device)\n",
        "\n",
        "model_bert = EnhancedDistilBERTClassifier(\n",
        "    num_classes=num_classes,\n",
        "    dropout=0.3,\n",
        "    freeze_bert=False\n",
        ").to(device)\n",
        "\n",
        "# Enhanced loss function with class weights\n",
        "class_weights_tensor = torch.tensor([class_weights[i] for i in range(num_classes)], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Enhanced optimizers with weight decay\n",
        "optimizer_lstm = optim.AdamW(model_lstm.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "optimizer_bert = optim.AdamW(model_bert.parameters(), lr=3e-5, weight_decay=1e-5)\n",
        "\n",
        "# Enhanced schedulers\n",
        "scheduler_lstm = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer_lstm, max_lr=2e-3,\n",
        "    steps_per_epoch=len(train_loader_lstm),\n",
        "    epochs=15\n",
        ")\n",
        "scheduler_bert = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer_bert, max_lr=3e-5,\n",
        "    steps_per_epoch=len(train_loader_bert),\n",
        "    epochs=6\n",
        ")\n",
        "\n",
        "# ------------------- 8. Enhanced Training Functions -------------------\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion,\n",
        "                epochs, model_name, evaluate_func, device, patience=4):\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    checkpoint_path = f\"checkpoints/best_{model_name.lower()}.pt\"\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            if model_name == \"LSTM\":\n",
        "                X_batch, y_batch = batch\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "            else:  # BERT\n",
        "                inputs, labels = batch\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "                y_batch = labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item() * y_batch.size(0)\n",
        "            total_samples += y_batch.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_samples\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_func(model, val_loader, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"[{model_name}] Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            patience_counter = 0\n",
        "            print(f\"  -> New best {model_name} model saved (Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"  -> Early stopping {model_name} (patience reached)\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    print(f\"Loaded best {model_name} model (Val Acc: {best_val_acc:.4f})\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "# Enhanced evaluation functions (same as before but with better metrics)\n",
        "def evaluate_lstm(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            total_loss += loss.item() * X_batch.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += X_batch.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate_bert(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "# Prediction functions for ensemble\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def predict_proba_lstm(model, data_loader, device):\n",
        "    model.eval()\n",
        "    probs_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            probs = softmax(outputs).cpu().numpy()\n",
        "            probs_list.append(probs)\n",
        "            labels_list.append(y_batch.numpy())\n",
        "    return np.vstack(probs_list), np.concatenate(labels_list)\n",
        "\n",
        "def predict_proba_bert(model, data_loader, device):\n",
        "    model.eval()\n",
        "    probs_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "            probs = softmax(outputs).cpu().numpy()\n",
        "            probs_list.append(probs)\n",
        "            labels_list.append(labels.numpy())\n",
        "    return np.vstack(probs_list), np.concatenate(labels_list)\n",
        "\n",
        "# ------------------- 9. Enhanced Training -------------------\n",
        "print(\"Training Enhanced LSTM Model...\")\n",
        "lstm_train_losses, lstm_val_losses, lstm_val_accs = train_model(\n",
        "    model_lstm, train_loader_lstm, val_loader_lstm,\n",
        "    optimizer_lstm, scheduler_lstm, criterion,\n",
        "    epochs=15, model_name=\"LSTM\", evaluate_func=evaluate_lstm, device=device\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Enhanced DistilBERT Model...\")\n",
        "bert_train_losses, bert_val_losses, bert_val_accs = train_model(\n",
        "    model_bert, train_loader_bert, val_loader_bert,\n",
        "    optimizer_bert, scheduler_bert, criterion,\n",
        "    epochs=6, model_name=\"BERT\", evaluate_func=evaluate_bert, device=device\n",
        ")\n",
        "\n",
        "# ------------------- 10. Enhanced Ensemble Strategy -------------------\n",
        "print(\"\\nOptimizing Ensemble Strategy...\")\n",
        "\n",
        "# Get validation predictions\n",
        "lstm_val_probs, val_labels_arr = predict_proba_lstm(model_lstm, val_loader_lstm, device)\n",
        "bert_val_probs, _ = predict_proba_bert(model_bert, val_loader_bert, device)\n",
        "\n",
        "# More sophisticated ensemble - try different combination strategies\n",
        "best_strategy = None\n",
        "best_acc = -1.0\n",
        "\n",
        "strategies = [\n",
        "    (\"weighted_average\", lambda w: w * lstm_val_probs + (1.0 - w) * bert_val_probs),\n",
        "    (\"weighted_geometric\", lambda w: np.power(lstm_val_probs, w) * np.power(bert_val_probs, (1.0 - w))),\n",
        "]\n",
        "\n",
        "for strategy_name, strategy_func in strategies:\n",
        "    for w in np.linspace(0.0, 1.0, 51):  # More granular search\n",
        "        try:\n",
        "            final_probs = strategy_func(w)\n",
        "            # Normalize probabilities\n",
        "            final_probs = final_probs / final_probs.sum(axis=1, keepdims=True)\n",
        "            preds = final_probs.argmax(axis=1)\n",
        "            acc = (preds == val_labels_arr).mean()\n",
        "\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                best_strategy = (strategy_name, w, strategy_func)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "strategy_name, best_w, best_func = best_strategy\n",
        "print(f\"Best ensemble strategy: {strategy_name} with w_lstm = {best_w:.3f}, val_acc = {best_acc:.4f}\")\n",
        "\n",
        "# ------------------- 11. Final Evaluation -------------------\n",
        "print(\"\\nEvaluating on Test Set...\")\n",
        "\n",
        "# Get test predictions\n",
        "lstm_test_probs, test_labels_arr = predict_proba_lstm(model_lstm, test_loader_lstm, device)\n",
        "bert_test_probs, _ = predict_proba_bert(model_bert, test_loader_bert, device)\n",
        "\n",
        "# Apply best ensemble strategy\n",
        "final_test_probs = best_func(best_w)\n",
        "if strategy_name == \"weighted_geometric\":\n",
        "    final_test_probs = final_test_probs / final_test_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "test_preds = final_test_probs.argmax(axis=1)\n",
        "test_acc = (test_preds == test_labels_arr).mean()\n",
        "\n",
        "print(f\"\\n=== FINAL RESULTS ===\")\n",
        "print(f\"Ensemble Test Accuracy: {100.0 * test_acc:.2f}%\")\n",
        "\n",
        "# Individual model accuracies\n",
        "_, lstm_test_acc = evaluate_lstm(model_lstm, test_loader_lstm, device)\n",
        "_, bert_test_acc = evaluate_bert(model_bert, test_loader_bert, device)\n",
        "\n",
        "print(f\"LSTM Test Accuracy: {100.0 * lstm_test_acc:.2f}%\")\n",
        "print(f\"DistilBERT Test Accuracy: {100.0 * bert_test_acc:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(test_labels_arr, test_preds, target_names=label_enc.classes_))\n",
        "\n",
        "# Plot training curves\n",
        "def plot_training_curves():\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # LSTM curves\n",
        "    axes[0, 0].plot(lstm_train_losses, label='Train Loss')\n",
        "    axes[0, 0].plot(lstm_val_losses, label='Val Loss')\n",
        "    axes[0, 0].set_title('LSTM Training/Validation Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    axes[0, 1].plot(lstm_val_accs, label='Val Accuracy')\n",
        "    axes[0, 1].set_title('LSTM Validation Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # BERT curves\n",
        "    axes[1, 0].plot(bert_train_losses, label='Train Loss')\n",
        "    axes[1, 0].plot(bert_val_losses, label='Val Loss')\n",
        "    axes[1, 0].set_title('DistilBERT Training/Validation Loss')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    axes[1, 1].plot(bert_val_accs, label='Val Accuracy')\n",
        "    axes[1, 1].set_title('DistilBERT Validation Accuracy')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_training_curves()\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Target Achieved: {'âœ…' if test_acc > 0.80 else 'âŒ'} (Target: >80%, Achieved: {100.0 * test_acc:.2f}%)\")"
      ]
    }
  ]
}